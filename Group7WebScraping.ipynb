{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Keltings/Soccer/blob/main/Group7WebScraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d2004e0",
      "metadata": {
        "id": "3d2004e0"
      },
      "outputs": [],
      "source": [
        "#downlaod the html of a page\n",
        "import requests\n",
        "#parsing html\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef83686e",
      "metadata": {
        "id": "ef83686e"
      },
      "outputs": [],
      "source": [
        "#define the url to start scraping\n",
        "standings_url = \"https://fbref.com/en/comps/9/Premier-League-Stats\"\n",
        "#standings_url = \"https://fbref.com/en/comps/9/2022-2023/2022-2023-Premier-League-Stats\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "213fc091",
      "metadata": {
        "id": "213fc091"
      },
      "outputs": [],
      "source": [
        "#downlaod the page\n",
        "data = requests.get(standings_url)\n",
        "#data.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "445653a7",
      "metadata": {
        "id": "445653a7"
      },
      "outputs": [],
      "source": [
        "#initialze beutifulSoup using html\n",
        "soup = BeautifulSoup(data.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0d2fa74",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "c0d2fa74",
        "outputId": "61a69a32-30f3-4ebb-db08-9703c2ae1a81"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-01fe80352a0b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# select the table using the css selector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstandings_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"table_container\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#select the first element in the list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Basically removed alot of the extra html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#standings_table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "# select the table using the css selector\n",
        "standings_table = soup.select(\"overall.stats_table\")[0]#select the first element in the list\n",
        "\n",
        "#Basically removed alot of the extra html\n",
        "#standings_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef115c17",
      "metadata": {
        "id": "ef115c17"
      },
      "outputs": [],
      "source": [
        "# find all of the a tags inside the table\n",
        "links = standings_table.find_all(\"a\")\n",
        "\n",
        "# now get the href property of each link\n",
        "#list comprehension\n",
        "links = [l.get('href') for l in links]\n",
        "\n",
        "#filter the links to get the squad links to give us stats for each individual squad\n",
        "links = [l for l in links if '/squads/' in l]\n",
        "links"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a2c9627",
      "metadata": {
        "id": "8a2c9627"
      },
      "source": [
        "Note that the `select` method uses css selectors which gives a lot of flexibility to select different elements, classes, ids, etc. Whereas `find_all` only find tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cf10f8e",
      "metadata": {
        "id": "7cf10f8e"
      },
      "outputs": [],
      "source": [
        "# turn our links into full urls\n",
        "team_url = [f'https://fbref.com{l}' for l in links]\n",
        "team_url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05f8bb38",
      "metadata": {
        "id": "05f8bb38"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "#for only Man-city\n",
        "team_url=team_url[0]\n",
        "data = requests.get(team_url)\n",
        "matches = pd.read_html(data.text, match='Scores & Fixtures')\n",
        "matches[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8460fdc1",
      "metadata": {
        "id": "8460fdc1"
      },
      "source": [
        "### get matches shooting stats with requests and pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e76f0d39",
      "metadata": {
        "id": "e76f0d39"
      },
      "outputs": [],
      "source": [
        "soup = BeautifulSoup(data.text)\n",
        "links = soup.find_all('a')\n",
        "\n",
        "#list comprehension\n",
        "links = [l.get('href') for l in links]\n",
        "\n",
        "#filter the links to get the squad links to give us stats for each individual squad\n",
        "links = [l for l in links if l and 'all_comps/shooting/' in l]\n",
        "links"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c401fb7",
      "metadata": {
        "id": "7c401fb7"
      },
      "outputs": [],
      "source": [
        "data=requests.get(f\"https://fbref.com{links[0]}\")\n",
        "shooting = pd.read_html(data.text, match='Shooting')[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "952b2bde",
      "metadata": {
        "id": "952b2bde"
      },
      "source": [
        "### Cleaning and merging scraped data in pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47a970cc",
      "metadata": {
        "id": "47a970cc"
      },
      "outputs": [],
      "source": [
        "shooting.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84806015",
      "metadata": {
        "id": "84806015"
      },
      "outputs": [],
      "source": [
        "# remove multilevel indexing ie columns with two names\n",
        "shooting.columns = shooting.columns.droplevel()\n",
        "shooting.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b208b73",
      "metadata": {
        "id": "7b208b73"
      },
      "outputs": [],
      "source": [
        "team_data = matches[0].merge(shooting[['Date', 'Sh', 'SoT', 'Dist', 'FK', 'PK', 'PKatt']], on='Date')\n",
        "team_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "804c3744",
      "metadata": {
        "id": "804c3744"
      },
      "outputs": [],
      "source": [
        "print(matches[0].shape)\n",
        "print(shooting.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fe5f086",
      "metadata": {
        "id": "3fe5f086"
      },
      "source": [
        "### Scaling the Method Up"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40c56d96",
      "metadata": {
        "id": "40c56d96"
      },
      "source": [
        "scrape data for multiple teams for multiple for multiple years"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ac3567f",
      "metadata": {
        "id": "1ac3567f"
      },
      "outputs": [],
      "source": [
        "# set up a list of the years we want to scrape\n",
        "years = list(range(2023,2019, -1))\n",
        "years"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69c9789c",
      "metadata": {
        "id": "69c9789c"
      },
      "outputs": [],
      "source": [
        "# initialize a list to contain several dataframes such that\n",
        "# each df contains the match logs for one team for each season\n",
        "all_matches = []\n",
        "\n",
        "# find the url we want to start on\n",
        "standings_url = \"https://fbref.com/en/comps/9/Premier-League-Stats\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7ae8b99",
      "metadata": {
        "id": "e7ae8b99"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "# loop through all the years\n",
        "#perform all the steps from our earlier codes\n",
        "#the codes we used to get  manchester\n",
        "for year in years:\n",
        "    data = requests.get(standings_url)\n",
        "    soup = BeautifulSoup(data.text)\n",
        "\n",
        "    #the table contains all of our teams individual links\n",
        "    standings_table = soup.select('table.stats_table')[0]\n",
        "\n",
        "    links = [l.get('href') for l in standings_table.find_all('a')]\n",
        "    links = [l for l in links if '/squads/' in l]\n",
        "    # turn from relative to absolute links\n",
        "    team_urls = [f\"https://fbref.com{l}\" for l in links]\n",
        "\n",
        "    #grab the url for the previous seasons\n",
        "    previous_season = soup.select('a.prev')[0].get('href')\n",
        "    standings_url = f'https://fbref.com/{previous_season}'\n",
        "\n",
        "    #loop through each of the team urls\n",
        "    for team_url in team_urls:\n",
        "        #individually scarpe the match logs for each team\n",
        "        #splitting on / we get the names of the team\n",
        "        team_name = team_url.split('/')[-1].replace('-Stats', '').replace(\"-\", \" \")\n",
        "        #print(team_url)\n",
        "        #print(team_url.split('/')[-1])\n",
        "        #print(team_name)\n",
        "\n",
        "        data = requests.get(team_url)\n",
        "        matches = pd.read_html(data.text, match=\"Scores & Fixtures\")[0]\n",
        "\n",
        "        #get for the shootings table\n",
        "        soup = BeautifulSoup(data.text)\n",
        "        links = [l.get('href') for l in soup.find_all('a')]\n",
        "        links = [l for l in links if l and 'all_comps/shooting/' in l]\n",
        "        data = requests.get(f'https://fbref.com{links[0]}')\n",
        "        shooting = pd.read_html(data.text, match='Shooting')[0]\n",
        "        shooting.columns=shooting.columns.droplevel()\n",
        "\n",
        "        try:\n",
        "          team_data = matches.merge(shooting[['Date', 'Sh', 'SoT', 'Dist', 'FK', 'PK', 'PKatt']], on='Date')\n",
        "\n",
        "      # filter our data for only competitoion was the premier league\n",
        "        # sometimes for some teams the shooting stats arent available so we wrap in\n",
        "        # a try and exception handle\n",
        "        # to not slow down the website\n",
        "        except ValueError:\n",
        "          continue\n",
        "        team_data = team_data[team_data['Comp'] == 'Premier League']\n",
        "        team_data['Season'] = year\n",
        "        team_data['Team'] = team_name\n",
        "        all_matches.append(team_data)\n",
        "\n",
        "        # to not scrape too quickly\n",
        "        time.sleep(3)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e75c62c6",
      "metadata": {
        "id": "e75c62c6"
      },
      "outputs": [],
      "source": [
        "print(all_matches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bfc07e8",
      "metadata": {
        "id": "2bfc07e8"
      },
      "outputs": [],
      "source": [
        "#combine all the dfs\n",
        "all_matches\n",
        "\n",
        "# all column names to be lower case\n",
        "all_matches.columns = [c.lower() for c in all_matches.columns]\n",
        "\n",
        "# write my data to csv file\n",
        "all_matches.to_csv('matches.csv')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "div",
      "language": "python",
      "name": "div"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}